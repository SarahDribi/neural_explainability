[INFO] Project root: /home/goofy/stage/modified-oval-bab/clean
[INFO] Experiments dir: /home/goofy/stage/modified-oval-bab/clean/experiments/coarsen_locally+20250812_100711
[INFO] Selected dir: clean/selected_images_explain_vcomp/L1+20250812_095334
[INFO] Using device: cpu
Loading ONNX model...
[INFO] Using label=1, epsilon=0.024901855468749998
[INFO] tensor shape=(784,), dtype=torch.float32, device=cpu
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.006911516189575195
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016815662384033203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001954793930053711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0021331310272216797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0024514198303222656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0026540756225585938

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
/home/goofy/stage/modified-oval-bab/plnn/anderson_linear_approximation.py:222: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(c_b, device=device).unsqueeze(0)
Coarsening NAP:   0%|          | 0/20 [00:00<?, ?neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.00796961784362793
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0021812915802001953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019834041595458984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0024814605712890625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002685546875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.003195047378540039

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (0,0) coarsened successfully.
Coarsening NAP:   5%|5         | 1/20 [00:00<00:03,  5.67neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.006340980529785156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0018310546875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0024192333221435547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0014638900756835938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008347034454345703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010516643524169922

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (0,1) coarsened successfully.
Coarsening NAP:  10%|#         | 2/20 [00:00<00:03,  5.88neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.003512859344482422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0026178359985351562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019288063049316406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0022649765014648438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002494335174560547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013327598571777344

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (0,2) coarsened successfully.
Coarsening NAP:  15%|#5        | 3/20 [00:00<00:02,  5.93neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.009669065475463867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016903877258300781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018093585968017578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0022144317626953125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0024924278259277344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0027539730072021484

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (0,3) coarsened successfully.
Coarsening NAP:  20%|##        | 4/20 [00:00<00:02,  5.60neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008849143981933594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005934238433837891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005812644958496094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0009882450103759766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008037090301513672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008776187896728516

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (0,4) coarsened successfully.
Coarsening NAP:  25%|##5       | 5/20 [00:00<00:02,  5.53neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.006996631622314453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0015511512756347656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019266605377197266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002191781997680664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0025801658630371094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0027763843536376953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (0,5) coarsened successfully.
Coarsening NAP:  30%|###       | 6/20 [00:01<00:02,  5.38neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0056569576263427734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016345977783203125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018610954284667969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002264738082885742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002473592758178711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0031654834747314453

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (0,6) coarsened successfully.
Coarsening NAP:  35%|###5      | 7/20 [00:01<00:02,  5.35neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.00441288948059082
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005424022674560547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006105899810791016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006608963012695312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0006756782531738281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008237361907958984

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (0,7) coarsened successfully.
Coarsening NAP:  40%|####      | 8/20 [00:01<00:02,  5.56neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005675077438354492
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006108283996582031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.000560760498046875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006055831909179688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007431507110595703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008440017700195312

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (0,8) coarsened successfully.
Coarsening NAP:  45%|####5     | 9/20 [00:01<00:02,  5.14neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.011104583740234375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005536079406738281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006122589111328125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006260871887207031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007715225219726562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008461475372314453

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (0,9) coarsened successfully.
Coarsening NAP:  50%|#####     | 10/20 [00:01<00:02,  4.85neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.002501964569091797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0028464794158935547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002095937728881836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0024678707122802734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002889871597290039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0019335746765136719

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Coarsening NAP:  55%|#####5    | 11/20 [00:02<00:01,  5.57neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005428791046142578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0008242130279541016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006268024444580078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007128715515136719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007951259613037109
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007915496826171875

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (1,1) coarsened successfully.
Coarsening NAP:  60%|######    | 12/20 [00:02<00:01,  5.05neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.007042407989501953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016448497772216797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019257068634033203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002179384231567383
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002534151077270508
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.003005504608154297

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (1,2) coarsened successfully.
Coarsening NAP:  65%|######5   | 13/20 [00:02<00:01,  4.68neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.01026463508605957
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.001638650894165039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019588470458984375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002206563949584961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0021126270294189453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010576248168945312

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (1,3) coarsened successfully.
Coarsening NAP:  70%|#######   | 14/20 [00:02<00:01,  4.53neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0071108341217041016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0012569427490234375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006356239318847656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006365776062011719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007624626159667969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008299350738525391

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (1,4) coarsened successfully.
Coarsening NAP:  75%|#######5  | 15/20 [00:02<00:01,  4.36neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005224466323852539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005774497985839844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006420612335205078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008862018585205078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008068084716796875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.00079345703125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Coarsening NAP:  80%|########  | 16/20 [00:03<00:00,  4.68neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0059850215911865234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.001514434814453125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0020639896392822266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0022919178009033203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0026731491088867188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014238357543945312

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (1,6) coarsened successfully.
Coarsening NAP:  85%|########5 | 17/20 [00:03<00:00,  4.40neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.01065516471862793
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0007672309875488281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007979869842529297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006721019744873047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007116794586181641
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008895397186279297

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Coarsening NAP:  90%|######### | 18/20 [00:03<00:00,  4.53neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0010991096496582031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.000530242919921875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005483627319335938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006015300750732422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007214546203613281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008966922760009766

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (1,8) coarsened successfully.
Coarsening NAP:  95%|#########5| 19/20 [00:03<00:00,  4.45neuron/s][INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.012106657028198242
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.005672931671142578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006494522094726562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006844997406005859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007932186126708984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008425712585449219

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Coarsening] Neuron (1,9) coarsened successfully.
Coarsening NAP: 100%|##########| 20/20 [00:04<00:00,  3.15neuron/s]Coarsening NAP: 100%|##########| 20/20 [00:04<00:00,  4.55neuron/s]
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0011684894561767578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005898475646972656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007762908935546875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007245540618896484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016853809356689453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0021827220916748047

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0016694068908691406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016574859619140625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0024993419647216797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002507448196411133
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002900838851928711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0017762184143066406

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 0 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0023865699768066406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006387233734130859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006480216979980469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006906986236572266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008554458618164062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009236335754394531

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 1 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0012750625610351562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0013098716735839844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013878345489501953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0016026496887207031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0017745494842529297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002079010009765625

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 2 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009491443634033203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0007340908050537109
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008559226989746094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0015676021575927734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0021355152130126953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.00394439697265625

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.001016855239868164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006358623504638672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007047653198242188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001020193099975586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0018987655639648438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0019659996032714844

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0015587806701660156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.002134561538696289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002432584762573242
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0027396678924560547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.003168821334838867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.003216981887817383

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0024504661560058594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0332036018371582
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014507770538330078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013184547424316406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010213851928710938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015978813171386719

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.001294851303100586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.001074075698852539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009348392486572266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0009531974792480469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013513565063476562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0018494129180908203

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0010809898376464844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0011675357818603516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0043942928314208984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0027556419372558594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.02197718620300293
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.007062673568725586

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0008389949798583984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005543231964111328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014662742614746094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.019574880599975586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.012617111206054688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0018649101257324219

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009222030639648438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005817413330078125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008289813995361328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011837482452392578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001356363296508789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013265609741210938

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0011873245239257812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0011470317840576172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013499259948730469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011324882507324219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013308525085449219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001600027084350586

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0008490085601806641
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005719661712646484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006258487701416016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006880760192871094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0015978813171386719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016400814056396484

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0007910728454589844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006008148193359375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008494853973388672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011115074157714844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012619495391845703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014536380767822266

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0010995864868164062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0008304119110107422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010557174682617188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010633468627929688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011570453643798828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016121864318847656

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0010139942169189453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0010678768157958984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009050369262695312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008656978607177734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008492469787597656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008876323699951172

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0011682510375976562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0020613670349121094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0020575523376464844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0023915767669677734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0025548934936523438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013713836669921875

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008270978927612305
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0017538070678710938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006425380706787109
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.000713348388671875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007159709930419922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007810592651367188

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009272098541259766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005793571472167969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005717277526855469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007023811340332031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007319450378417969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007898807525634766

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009148120880126953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005474090576171875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001809835433959961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0028123855590820312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0027730464935302734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001018524169921875

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0008864402770996094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005774497985839844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007977485656738281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008490085601806641
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008869171142578125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009980201721191406

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009424686431884766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0014660358428955078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013072490692138672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.004144191741943359
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013117790222167969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008080005645751953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.001554250717163086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0023162364959716797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0025501251220703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002583742141723633
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001379251480102539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012905597686767578

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009186267852783203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005519390106201172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005517005920410156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007033348083496094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008313655853271484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007936954498291016

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0008962154388427734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005371570587158203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006279945373535156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007178783416748047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0006976127624511719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008325576782226562

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.002061128616333008
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.000530242919921875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006101131439208984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006122589111328125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0006737709045410156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008935928344726562

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.006717205047607422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0018310546875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0020384788513183594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0024290084838867188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002813577651977539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.003613710403442383

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.012179851531982422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0007302761077880859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.00064849853515625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007481575012207031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007612705230712891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007824897766113281

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.009564638137817383
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.005517005920410156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006821155548095703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006968975067138672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007269382476806641
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008640289306640625

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005706310272216797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005776882171630859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013463497161865234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0023865699768066406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0024552345275878906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0029578208923339844

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.006575107574462891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016019344329833984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010106563568115234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007612705230712891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009217262268066406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007894039154052734

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0008783340454101562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0009250640869140625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001310110092163086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001359701156616211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0015649795532226562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008325576782226562

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.003942966461181641
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006406307220458984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006778240203857422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011279582977294922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011518001556396484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009291172027587891

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0012640953063964844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005488395690917969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005619525909423828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006587505340576172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0006923675537109375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007939338684082031

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.00479578971862793
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005803108215332031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005908012390136719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006680488586425781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007228851318359375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.00098419189453125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.010564088821411133
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0015158653259277344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0021152496337890625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0026214122772216797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0026254653930664062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013396739959716797

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.006078243255615234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016617774963378906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018475055694580078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0021393299102783203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002500772476196289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0027747154235839844

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.009046316146850586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0018470287322998047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0021371841430664062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0023145675659179688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002349376678466797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009484291076660156

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0041005611419677734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006437301635742188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007147789001464844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006928443908691406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009009838104248047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008220672607421875

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008296728134155273
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016434192657470703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019459724426269531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.00115966796875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008389949798583984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008442401885986328

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0064318180084228516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.002045154571533203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0022530555725097656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0023221969604492188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002539396286010742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002027750015258789

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005491733551025391
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016477108001708984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.00279998779296875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002702474594116211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0020852088928222656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007944107055664062

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0046787261962890625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.001592397689819336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006132125854492188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006234645843505859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001116037368774414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009353160858154297

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008628129959106445
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0015614032745361328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002044677734375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002191781997680664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002626180648803711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002888917922973633

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.010607481002807617
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0015511512756347656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018324851989746094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002168893814086914
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0025594234466552734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002893686294555664

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005757808685302734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0007982254028320312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007581710815429688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006115436553955078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007143020629882812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007739067077636719

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008319854736328125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006463527679443359
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008990764617919922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007798671722412109
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007534027099609375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008563995361328125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0008540153503417969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0007712841033935547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008065700531005859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0009560585021972656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010783672332763672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011370182037353516

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.00979304313659668
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0015387535095214844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0017518997192382812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002470731735229492
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0032596588134765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010192394256591797

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.002357959747314453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005691051483154297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005588531494140625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008184909820556641
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0006992816925048828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009686946868896484

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008496761322021484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0018863677978515625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001954317092895508
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002332925796508789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0027108192443847656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0028367042541503906

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009133815765380859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005943775177001953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006291866302490234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006930828094482422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007390975952148438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007686614990234375

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.003566265106201172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0007441043853759766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007390975952148438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010790824890136719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011115074157714844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002506732940673828

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.004086017608642578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0015649795532226562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019292831420898438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0021708011627197266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0022597312927246094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009312629699707031

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.010483741760253906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016100406646728516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019049644470214844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002275228500366211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0025625228881835938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002942800521850586

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0071637630462646484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0017521381378173828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018410682678222656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002275228500366211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0029687881469726562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010061264038085938

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.009645700454711914
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016434192657470703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018582344055175781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0024394989013671875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0026001930236816406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016357898712158203

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.011283159255981445
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016536712646484375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011363029479980469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007948875427246094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007886886596679688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009310245513916016

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.00629425048828125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.002531290054321289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0023453235626220703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0024268627166748047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0025506019592285156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0039594173431396484

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0010936260223388672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0008237361907958984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008318424224853516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010242462158203125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011506080627441406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012962818145751953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.006632089614868164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0023200511932373047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002173185348510742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0022389888763427734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0025806427001953125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0036897659301757812

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0008406639099121094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006132125854492188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005514621734619141
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006034374237060547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011227130889892578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.00118255615234375

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0011022090911865234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006830692291259766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005719661712646484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007874965667724609
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007042884826660156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011508464813232422

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.011504650115966797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0023708343505859375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018818378448486328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0021440982818603516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013980865478515625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010216236114501953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.010454177856445312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.002765178680419922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006403923034667969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007190704345703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007219314575195312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008597373962402344

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.003452777862548828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0013346672058105469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007688999176025391
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008542537689208984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007710456848144531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010724067687988281

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.012051105499267578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.000606536865234375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005712509155273438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008594989776611328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0006940364837646484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007953643798828125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.007349729537963867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006899833679199219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006029605865478516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006296634674072266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007293224334716797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008618831634521484

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0008783340454101562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006165504455566406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005600452423095703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.000598907470703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007317066192626953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008585453033447266

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.007841348648071289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0015194416046142578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0020270347595214844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0014302730560302734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012421607971191406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013456344604492188

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008372306823730469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0017600059509277344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018742084503173828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0021867752075195312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0026116371154785156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011305809020996094

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.012552738189697266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.001561880111694336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019249916076660156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0022115707397460938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002225160598754883
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011112689971923828

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.007124423980712891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0015897750854492188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0021889209747314453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0023980140686035156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0026454925537109375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0023946762084960938

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.004483222961425781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006341934204101562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005819797515869141
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006213188171386719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007853507995605469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007970333099365234

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.012631416320800781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.004033088684082031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001958131790161133
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0019545555114746094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009829998016357422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009558200836181641

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0008625984191894531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005850791931152344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005848407745361328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006048679351806641
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007562637329101562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007901191711425781

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.017214536666870117
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0007336139678955078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006449222564697266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007846355438232422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007741451263427734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008141994476318359

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.012083292007446289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0018877983093261719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002382040023803711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002432584762573242
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.003082275390625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0034151077270507812

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008695602416992188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0007491111755371094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006010532379150391
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0009980201721191406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001007080078125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010325908660888672

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.010333776473999023
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016179084777832031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018362998962402344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0021898746490478516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0026683807373046875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001833200454711914

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005300045013427734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005958080291748047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006415843963623047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006744861602783203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0006866455078125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007910728454589844

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0027909278869628906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.002618074417114258
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0022377967834472656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0021669864654541016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0025374889373779297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013027191162109375

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.013266324996948242
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0034346580505371094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019130706787109375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0023386478424072266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0026984214782714844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001276254653930664

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.002069711685180664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005781650543212891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006139278411865234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006227493286132812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0006744861602783203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008389949798583984

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.024266958236694336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0033218860626220703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005726814270019531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006680488586425781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007693767547607422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008738040924072266

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005972146987915039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005807876586914062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0005447864532470703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0005929470062255859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00074005126953125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0029518604278564453

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.023699045181274414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.001661062240600586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019330978393554688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002277374267578125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0025925636291503906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001825571060180664

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.010174989700317383
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.002166748046875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001893758773803711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0022504329681396484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0027129650115966797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0030858516693115234

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02517986297607422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0012896060943603516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002214670181274414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0018579959869384766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007991790771484375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009169578552246094

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.00543975830078125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0018794536590576172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0021986961364746094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010077953338623047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008664131164550781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009031295776367188

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.006838083267211914
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006055831909179688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007174015045166016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007846355438232422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007078647613525391
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008411407470703125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008113861083984375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0015530586242675781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002095937728881836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0022110939025878906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002597808837890625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015044212341308594

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008952856063842773
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005948543548583984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009326934814453125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006892681121826172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007836818695068359
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007796287536621094

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005425930023193359
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.002620220184326172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0024003982543945312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0020830631256103516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007338523864746094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008242130279541016

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.01325368881225586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0018398761749267578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0017747879028320312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0022449493408203125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00250244140625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0028700828552246094

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008334159851074219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0017437934875488281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002039194107055664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011584758758544922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009677410125732422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010571479797363281

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0051403045654296875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.001615285873413086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002070188522338867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0021178722381591797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002482891082763672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0028886795043945312

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0034291744232177734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0028400421142578125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019338130950927734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001173257827758789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007085800170898438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008192062377929688

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.010853290557861328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006711483001708984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007221698760986328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007181167602539062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007233619689941406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0007586479187011719

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.011881828308105469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.001718759536743164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0020380020141601562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002397775650024414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002613067626953125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0029113292694091797

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.6393, max=5.4329 | UB min=-3.1373, max=8.7757
  Layer 2: LB min=-6.5862, max=5.2590 | UB min=-1.3061, max=10.8629
  Layer 3: LB min=-14.0311, max=19.9157 | UB min=-4.0999, max=35.9390
  Layer 4: LB min=-9.9958, max=24.1029 | UB min=5.2035, max=35.6224
  Layer 5: LB min=-12.6331, max=24.5278 | UB min=8.7639, max=39.9187
  Layer 6: LB min=-1.8840, max=19.9157 | UB min=23.7423, max=35.5275
  Layer 7: LB min=-9.3898, max=-9.3898 | UB min=10.6073, max=10.6073
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Stochastic] Coarsened neurons: 9 | Remaining %: 55.00 | successful_iterations=3
[INFO] Coarsened NAP (stochastic): [[1, -1, -1, -1, -1, 1, 1, 1, -1, -1], [0, 1, 1, 1, -1, 0, 1, -1, 0, -1]]
[INFO] Original NAP:               [[1, 1, 1, 0, 0, 1, 1, 1, 1, 1], [0, 1, 1, 1, 1, 0, 1, 1, 0, 1]]
[OK] Results saved to /home/goofy/stage/modified-oval-bab/clean/experiments/coarsen_locally+20250812_100711/results.json
