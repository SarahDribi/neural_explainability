[INFO] Project root: /home/goofy/stage/modified-oval-bab/clean
[INFO] Experiments dir: /home/goofy/stage/modified-oval-bab/clean/experiments/first_script+20250811_131819
Loading ONNX model...
[INFO] Searching for NAP-exclusive robust inputs for label 7
Loading ONNX model...
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.04058957099914551
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.031218767166137695
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0017275810241699219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.014397382736206055
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004126310348510742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.009252071380615234

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-29.4761, max=0.0000 | UB min=0.0000, max=54.0567
  Layer 2: LB min=-62.1687, max=0.0000 | UB min=0.0000, max=88.2133
  Layer 3: LB min=-254.9277, max=227.6115 | UB min=149.3632, max=625.0234
  Layer 4: LB min=-222.7750, max=353.6494 | UB min=187.2682, max=876.2531
  Layer 5: LB min=-311.9875, max=372.4767 | UB min=323.9791, max=990.0135
  Layer 6: LB min=-151.0905, max=195.6768 | UB min=591.9585, max=615.0369
  Layer 7: LB min=-550.3249, max=-550.3249 | UB min=172.0707, max=172.0707
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
/home/goofy/stage/modified-oval-bab/plnn/anderson_linear_approximation.py:222: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  return torch.tensor(c_b, device=device).unsqueeze(0)
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.043302297592163086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.010438919067382812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011878013610839844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0062024593353271484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016999244689941406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004760265350341797

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-17.2585, max=0.0000 | UB min=0.0000, max=28.8259
  Layer 2: LB min=-34.7762, max=0.0000 | UB min=0.0000, max=47.7934
  Layer 3: LB min=-131.1515, max=128.7887 | UB min=78.2055, max=333.1095
  Layer 4: LB min=-115.4493, max=199.7521 | UB min=96.6544, max=469.8890
  Layer 5: LB min=-161.1712, max=209.9439 | UB min=165.8873, max=528.0155
  Layer 6: LB min=-81.9466, max=114.6019 | UB min=300.4210, max=328.0734
  Layer 7: LB min=-281.8444, max=-281.8444 | UB min=90.4717, max=90.4717
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0447688102722168
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.010434865951538086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0065729618072509766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0015072822570800781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0040242671966552734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015439987182617188

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-10.9288, max=0.0000 | UB min=0.0000, max=15.9970
  Layer 2: LB min=-18.9231, max=0.0000 | UB min=0.0000, max=26.1041
  Layer 3: LB min=-62.2757, max=80.5453 | UB min=40.0494, max=179.2475
  Layer 4: LB min=-57.0350, max=126.0738 | UB min=46.1015, max=253.9731
  Layer 5: LB min=-77.1013, max=128.6959 | UB min=78.3242, max=281.5080
  Layer 6: LB min=-36.8459, max=76.4599 | UB min=138.9855, max=171.1921
  Layer 7: LB min=-129.4529, max=-129.4529 | UB min=46.0374, max=46.0374
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0040132999420166016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.007245779037475586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0036988258361816406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0037708282470703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001810312271118164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0019383430480957031

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-7.6539, max=3.0002 | UB min=0.0000, max=11.6344
  Layer 2: LB min=-12.0903, max=4.0506 | UB min=0.0000, max=17.5873
  Layer 3: LB min=-32.8712, max=49.3822 | UB min=13.6384, max=90.6606
  Layer 4: LB min=-26.7409, max=80.0569 | UB min=18.9120, max=131.2526
  Layer 5: LB min=-31.1489, max=79.5355 | UB min=31.5667, max=142.4159
  Layer 6: LB min=-20.5975, max=49.3822 | UB min=50.6275, max=88.7357
  Layer 7: LB min=-47.4459, max=-47.4459 | UB min=24.5203, max=24.5203
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027179241180419922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.022805213928222656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011494159698486328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011019706726074219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012156963348388672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.006951093673706055

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-5.9796, max=5.1043 | UB min=-1.1794, max=9.5379
  Layer 2: LB min=-9.3219, max=7.6560 | UB min=-2.5831, max=14.1717
  Layer 3: LB min=-22.0668, max=27.0749 | UB min=0.3125, max=45.8806
  Layer 4: LB min=-14.6754, max=46.3850 | UB min=7.3874, max=66.8269
  Layer 5: LB min=-12.1393, max=46.6613 | UB min=12.8536, max=71.2982
  Layer 6: LB min=-15.1320, max=27.0749 | UB min=14.9221, max=45.8806
  Layer 7: LB min=-14.8371, max=-14.8371 | UB min=13.1094, max=13.1094
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008010149002075195
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.009812116622924805
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.004656553268432617
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001497507095336914
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002239704132080078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0021979808807373047

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-5.1305, max=6.1569 | UB min=-2.6166, max=8.4814
  Layer 2: LB min=-7.8218, max=9.1534 | UB min=-4.2690, max=12.6033
  Layer 3: LB min=-16.8277, max=16.3297 | UB min=-5.2878, max=25.8043
  Layer 4: LB min=-9.0626, max=30.3634 | UB min=2.2936, max=40.3912
  Layer 5: LB min=-5.9595, max=30.3634 | UB min=5.2124, max=41.3534
  Layer 6: LB min=-11.2717, max=16.3297 | UB min=1.3101, max=25.8043
  Layer 7: LB min=-2.4310, max=-2.4310 | UB min=7.7569, max=7.7569
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.014311075210571289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.014440774917602539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010821819305419922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012047290802001953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001361846923828125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014386177062988281

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.6917, max=6.6866 | UB min=-3.3408, max=7.9372
  Layer 2: LB min=-7.0462, max=9.9182 | UB min=-5.1258, max=11.8012
  Layer 3: LB min=-14.4785, max=10.1406 | UB min=-8.1585, max=14.8667
  Layer 4: LB min=-5.7259, max=22.5343 | UB min=-0.0634, max=27.6266
  Layer 5: LB min=-3.6561, max=22.5491 | UB min=1.5908, max=28.0140
  Layer 6: LB min=-9.6761, max=10.1406 | UB min=-4.3071, max=14.8667
  Layer 7: LB min=0.3088, max=0.3088 | UB min=5.0349, max=5.0349
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0013577938079833984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0018990039825439453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010917186737060547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.00246429443359375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001552581787109375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0021886825561523438

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.9174, max=6.4200 | UB min=-2.9763, max=8.2162
  Layer 2: LB min=-7.4433, max=9.5301 | UB min=-4.6911, max=12.2096
  Layer 3: LB min=-15.5671, max=13.8830 | UB min=-6.7046, max=21.0294
  Layer 4: LB min=-7.4078, max=26.8845 | UB min=1.0787, max=34.4684
  Layer 5: LB min=-4.6644, max=26.9696 | UB min=3.2454, max=34.9521
  Layer 6: LB min=-10.4868, max=13.8830 | UB min=-1.7607, max=21.0294
  Layer 7: LB min=-0.7281, max=-0.7281 | UB min=6.4183, max=6.4183
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027762651443481445
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.022080421447753906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016815662384033203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0016736984252929688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016279220581054688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0017066001892089844

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.8065, max=6.5528 | UB min=-3.1578, max=8.0789
  Layer 2: LB min=-7.2478, max=9.7222 | UB min=-4.9064, max=12.0186
  Layer 3: LB min=-15.2010, max=12.0300 | UB min=-7.4256, max=17.9710
  Layer 4: LB min=-6.5925, max=24.7327 | UB min=0.4737, max=31.1201
  Layer 5: LB min=-4.2387, max=24.7661 | UB min=2.4045, max=31.5987
  Layer 6: LB min=-10.0853, max=12.0300 | UB min=-3.0165, max=17.9710
  Layer 7: LB min=-0.2079, max=-0.2079 | UB min=5.7331, max=5.7331
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.012913703918457031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.031237363815307617
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014085769653320312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011317729949951172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002789735794067383
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016112327575683594

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.7491, max=6.6197 | UB min=-3.2493, max=8.0080
  Layer 2: LB min=-7.1472, max=9.8199 | UB min=-5.0161, max=11.9099
  Layer 3: LB min=-14.8409, max=11.0852 | UB min=-7.7922, max=16.4187
  Layer 4: LB min=-6.1594, max=23.6335 | UB min=0.1711, max=29.3751
  Layer 5: LB min=-3.9346, max=23.6547 | UB min=1.9761, max=29.8065
  Layer 6: LB min=-9.8806, max=11.0852 | UB min=-3.6882, max=16.4187
  Layer 7: LB min=0.0506, max=0.0506 | UB min=5.3841, max=5.3841
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.04390406608581543
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018935203552246094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0029997825622558594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.004520416259765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.009242534637451172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004003763198852539

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.7778, max=6.5862 | UB min=-3.2036, max=8.0435
  Layer 2: LB min=-7.1976, max=9.7710 | UB min=-4.9613, max=11.9643
  Layer 3: LB min=-15.0212, max=11.5576 | UB min=-7.6089, max=17.1949
  Layer 4: LB min=-6.3760, max=24.1831 | UB min=0.3175, max=30.2492
  Layer 5: LB min=-4.0858, max=24.2098 | UB min=2.1870, max=30.7026
  Layer 6: LB min=-9.9829, max=11.5576 | UB min=-3.3561, max=17.1949
  Layer 7: LB min=-0.0786, max=-0.0786 | UB min=5.5586, max=5.5586
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.020974159240722656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021291255950927734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.005432844161987305
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0033528804779052734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0037550926208496094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.010051727294921875

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.7635, max=6.6030 | UB min=-3.2264, max=8.0257
  Layer 2: LB min=-7.1724, max=9.7954 | UB min=-4.9887, max=11.9371
  Layer 3: LB min=-14.9311, max=11.3214 | UB min=-7.7006, max=16.8068
  Layer 4: LB min=-6.2677, max=23.9083 | UB min=0.2395, max=29.8121
  Layer 5: LB min=-4.0091, max=23.9321 | UB min=2.0798, max=30.2545
  Layer 6: LB min=-9.9318, max=11.3214 | UB min=-3.5243, max=16.8068
  Layer 7: LB min=-0.0140, max=-0.0140 | UB min=5.4714, max=5.4714
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.001028299331665039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0019314289093017578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008029937744140625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007932186126708984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009431838989257812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010149478912353516

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.7563, max=6.6113 | UB min=-3.2379, max=8.0169
  Layer 2: LB min=-7.1598, max=9.8077 | UB min=-5.0024, max=11.9235
  Layer 3: LB min=-14.8861, max=11.2033 | UB min=-7.7464, max=16.6128
  Layer 4: LB min=-6.2136, max=23.7709 | UB min=0.2006, max=29.5936
  Layer 5: LB min=-3.9706, max=23.7933 | UB min=2.0265, max=30.0305
  Layer 6: LB min=-9.9062, max=11.2033 | UB min=-3.6079, max=16.6128
  Layer 7: LB min=0.0183, max=0.0183 | UB min=5.4277, max=5.4277
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009684562683105469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006525516510009766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006518363952636719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0028743743896484375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010344982147216797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014803409576416016

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.7599, max=6.6071 | UB min=-3.2322, max=8.0213
  Layer 2: LB min=-7.1661, max=9.8016 | UB min=-4.9956, max=11.9303
  Layer 3: LB min=-14.9086, max=11.2624 | UB min=-7.7235, max=16.7098
  Layer 4: LB min=-6.2407, max=23.8396 | UB min=0.2200, max=29.7029
  Layer 5: LB min=-3.9899, max=23.8627 | UB min=2.0531, max=30.1425
  Layer 6: LB min=-9.9190, max=11.2624 | UB min=-3.5661, max=16.7098
  Layer 7: LB min=0.0022, max=0.0022 | UB min=5.4495, max=5.4495
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005311489105224609
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.010892391204833984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0029184818267822266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.003590822219848633
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00723576545715332
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0050928592681884766

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.7599, max=6.6071 | UB min=-3.2322, max=8.0213
  Layer 2: LB min=-7.1661, max=9.8016 | UB min=-4.9956, max=11.9303
  Layer 3: LB min=-14.9086, max=11.2624 | UB min=-7.7235, max=16.7098
  Layer 4: LB min=-6.2407, max=23.8396 | UB min=0.2200, max=29.7029
  Layer 5: LB min=-3.9899, max=23.8627 | UB min=2.0531, max=30.1425
  Layer 6: LB min=-9.9190, max=11.2624 | UB min=-3.5661, max=16.7098
  Layer 7: LB min=0.0022, max=0.0022 | UB min=5.4495, max=5.4495
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.002561330795288086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0022220611572265625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.006433725357055664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.004355430603027344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00459599494934082
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012977123260498047
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.000949859619140625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0007147789001464844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.012846708297729492
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.003467559814453125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016322135925292969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012822151184082031
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0010974407196044922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0014667510986328125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013294219970703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013408660888671875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0017092227935791016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.005813121795654297
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.013747692108154297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01022481918334961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.006679058074951172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0037195682525634766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.011788368225097656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0059549808502197266
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.003412961959838867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0039904117584228516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010786056518554688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.004603147506713867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013000965118408203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011677742004394531
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.007694244384765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.004169940948486328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002886533737182617
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.00844883918762207
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00284576416015625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0023345947265625
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009276866912841797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0017647743225097656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007524490356445312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008411407470703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008766651153564453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0025911331176757812
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0013728141784667969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0016028881072998047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012981891632080078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012292861938476562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0014498233795166016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016391277313232422
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.009782791137695312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0027201175689697266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.005896806716918945
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0038280487060546875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.006982326507568359
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0056684017181396484
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.012007951736450195
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0058231353759765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.007643222808837891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011782646179199219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013873577117919922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011682510375976562
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.014246940612792969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0022830963134765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0017459392547607422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.009166717529296875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0035316944122314453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0022077560424804688
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.005984306335449219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.004286527633666992
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.003072500228881836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.003502368927001953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.010286331176757812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004842042922973633
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.015815019607543945
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015456438064575195
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001966714859008789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002590656280517578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004852771759033203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0023653507232666016
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.006739377975463867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0018131732940673828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007870197296142578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010123252868652344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009303092956542969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.006206035614013672
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.009889602661132812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.006542205810546875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0036840438842773438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.006093502044677734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0073354244232177734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.00861501693725586
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
Loading ONNX model...
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03455948829650879
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02208733558654785
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.007010936737060547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.012799501419067383
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.015120744705200195
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.005118608474731445

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-22.7939, max=0.0000 | UB min=0.0000, max=43.4700
  Layer 2: LB min=-59.4250, max=0.0000 | UB min=0.0000, max=90.8436
  Layer 3: LB min=-250.6546, max=222.9318 | UB min=149.9642, max=618.7949
  Layer 4: LB min=-221.9346, max=345.5855 | UB min=186.2290, max=863.9557
  Layer 5: LB min=-308.6651, max=363.4973 | UB min=321.9206, max=975.5804
  Layer 6: LB min=-148.1313, max=189.1578 | UB min=589.5392, max=608.1545
  Layer 7: LB min=-546.1936, max=-546.1936 | UB min=170.2047, max=170.2047
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008934259414672852
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.003403902053833008
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0032830238342285156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0061604976654052734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.008208990097045898
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0022084712982177734

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-13.8308, max=0.0000 | UB min=0.0000, max=22.6731
  Layer 2: LB min=-31.2556, max=0.0000 | UB min=0.0000, max=46.5720
  Layer 3: LB min=-126.8336, max=122.2488 | UB min=81.0038, max=328.3250
  Layer 4: LB min=-116.3000, max=189.3011 | UB min=97.2832, max=458.4485
  Layer 5: LB min=-160.4994, max=198.2835 | UB min=166.1888, max=514.8871
  Layer 6: LB min=-79.3742, max=104.3946 | UB min=304.5440, max=322.2051
  Layer 7: LB min=-282.6085, max=-282.6085 | UB min=89.1852, max=89.1852
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.002340555191040039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0015358924865722656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.009103536605834961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.006993293762207031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0028951168060302734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0029752254486083984

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-9.1462, max=0.0000 | UB min=0.0000, max=12.8835
  Layer 2: LB min=-17.2713, max=0.0000 | UB min=0.0000, max=25.4278
  Layer 3: LB min=-65.6530, max=72.1847 | UB min=44.0771, max=177.4863
  Layer 4: LB min=-59.8431, max=110.4204 | UB min=50.1955, max=247.8884
  Layer 5: LB min=-81.6710, max=115.0683 | UB min=83.7670, max=275.2968
  Layer 6: LB min=-42.6372, max=62.5130 | UB min=154.6007, max=173.6433
  Layer 7: LB min=-142.4731, max=-142.4731 | UB min=37.1422, max=37.1422
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.009727001190185547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.008347511291503906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.007191896438598633
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0019266605377197266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0021605491638183594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0023152828216552734

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.7089, max=0.0000 | UB min=0.0000, max=8.2663
  Layer 2: LB min=-9.3269, max=0.0000 | UB min=0.0000, max=15.1982
  Layer 3: LB min=-30.0409, max=49.8926 | UB min=18.4572, max=96.2603
  Layer 4: LB min=-27.5306, max=75.6431 | UB min=23.1642, max=135.9514
  Layer 5: LB min=-36.8311, max=75.8226 | UB min=36.1534, max=146.8641
  Layer 6: LB min=-16.6347, max=43.5501 | UB min=70.0198, max=93.1597
  Layer 7: LB min=-60.4859, max=-60.4859 | UB min=22.7082, max=22.7082
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.006341695785522461
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.012274503707885742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.017467498779296875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.008292913436889648
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.005284786224365234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0018451213836669922

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-5.4670, max=1.6936 | UB min=-0.5757, max=5.9463
  Layer 2: LB min=-5.8774, max=3.4671 | UB min=0.0000, max=11.1457
  Layer 3: LB min=-15.3136, max=32.2539 | UB min=3.3667, max=50.6130
  Layer 4: LB min=-11.2072, max=47.1176 | UB min=9.9478, max=70.0946
  Layer 5: LB min=-14.9598, max=47.1176 | UB min=12.8443, max=74.2771
  Layer 6: LB min=-5.5065, max=26.2792 | UB min=29.1211, max=46.6766
  Layer 7: LB min=-20.8367, max=-20.8367 | UB min=12.1494, max=12.1494
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.012767553329467773
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.017910480499267578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.005425453186035156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.005955219268798828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.007112264633178711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.006236076354980469

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.8308, max=2.6613 | UB min=-2.2988, max=4.8623
  Layer 2: LB min=-4.3167, max=5.5094 | UB min=-0.7677, max=9.4099
  Layer 3: LB min=-10.1153, max=19.5642 | UB min=-0.2078, max=28.5184
  Layer 4: LB min=-5.3201, max=27.8793 | UB min=4.4199, max=38.7402
  Layer 5: LB min=-6.8113, max=27.8793 | UB min=5.6589, max=40.8060
  Layer 6: LB min=-0.7743, max=13.3327 | UB min=15.3828, max=23.7448
  Layer 7: LB min=-7.0236, max=-7.0236 | UB min=7.8788, max=7.8788
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.003009319305419922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.002844572067260742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016281604766845703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0017237663269042969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0022428035736083984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002605915069580078

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.5098, max=3.1476 | UB min=-3.1613, max=4.3202
  Layer 2: LB min=-3.5352, max=6.4764 | UB min=-1.6847, max=8.5262
  Layer 3: LB min=-7.5340, max=10.9413 | UB min=-2.4583, max=15.5180
  Layer 4: LB min=-2.7832, max=15.2274 | UB min=1.8420, max=20.3225
  Layer 5: LB min=-2.7272, max=15.2274 | UB min=2.5614, max=21.2269
  Layer 6: LB min=1.5521, max=7.1188 | UB min=9.0762, max=12.5259
  Layer 7: LB min=-1.1294, max=-1.1294 | UB min=5.6897, max=5.6897
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0013954639434814453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0008556842803955078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009474754333496094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010335445404052734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001245260238647461
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012440681457519531

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.6705, max=2.9042 | UB min=-2.7301, max=4.5912
  Layer 2: LB min=-3.9261, max=5.9926 | UB min=-1.2264, max=8.9680
  Layer 3: LB min=-8.8235, max=15.6581 | UB min=-1.3618, max=22.4172
  Layer 4: LB min=-4.0098, max=21.9814 | UB min=3.1085, max=29.9736
  Layer 5: LB min=-4.7521, max=21.9814 | UB min=4.1151, max=31.4505
  Layer 6: LB min=0.4048, max=10.1691 | UB min=12.2252, max=18.0634
  Layer 7: LB min=-3.9960, max=-3.9960 | UB min=6.7981, max=6.7981
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.014729738235473633
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015539884567260742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.018303871154785156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.00889134407043457
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002493143081665039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015041828155517578

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.5902, max=3.0259 | UB min=-2.9457, max=4.4557
  Layer 2: LB min=-3.7307, max=6.2344 | UB min=-1.4557, max=8.7471
  Layer 3: LB min=-8.1783, max=13.2431 | UB min=-1.9183, max=18.9096
  Layer 4: LB min=-3.3786, max=18.4902 | UB min=2.4661, max=25.0266
  Layer 5: LB min=-3.7312, max=18.4902 | UB min=3.3319, max=26.2106
  Layer 6: LB min=0.9839, max=8.5708 | UB min=10.6364, max=15.2148
  Layer 7: LB min=-2.5509, max=-2.5509 | UB min=6.2606, max=6.2606
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.014044046401977539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01897740364074707
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.009595632553100586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0094757080078125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0022733211517333984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016641616821289062

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.5500, max=3.0867 | UB min=-3.0535, max=4.3880
  Layer 2: LB min=-3.6330, max=6.3554 | UB min=-1.5702, max=8.6367
  Layer 3: LB min=-7.8559, max=12.0337 | UB min=-2.1862, max=17.1553
  Layer 4: LB min=-3.0811, max=16.7427 | UB min=2.1532, max=22.5571
  Layer 5: LB min=-3.2293, max=16.7427 | UB min=2.9452, max=23.6012
  Layer 6: LB min=1.2681, max=7.7689 | UB min=9.8564, max=13.7948
  Layer 7: LB min=-1.8392, max=-1.8392 | UB min=5.9838, max=5.9838
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0019974708557128906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0009248256683349609
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010046958923339844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.012189865112304688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.01941514015197754
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002883434295654297

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.5299, max=3.1171 | UB min=-3.1074, max=4.3541
  Layer 2: LB min=-3.5841, max=6.4159 | UB min=-1.6275, max=8.5815
  Layer 3: LB min=-7.6949, max=11.4386 | UB min=-2.3216, max=16.2877
  Layer 4: LB min=-2.9328, max=15.8876 | UB min=1.9975, max=21.3421
  Layer 5: LB min=-2.9784, max=15.8876 | UB min=2.7528, max=22.3163
  Layer 6: LB min=1.4102, max=7.3805 | UB min=9.4646, max=13.0972
  Layer 7: LB min=-1.4841, max=-1.4841 | UB min=5.8395, max=5.8395
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0036678314208984375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.009034872055053711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002828359603881836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0014774799346923828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001514434814453125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012488365173339844

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.5199, max=3.1324 | UB min=-3.1344, max=4.3372
  Layer 2: LB min=-3.5597, max=6.4461 | UB min=-1.6561, max=8.5538
  Layer 3: LB min=-7.6145, max=11.1900 | UB min=-2.3898, max=15.9028
  Layer 4: LB min=-2.8583, max=15.5575 | UB min=1.9197, max=20.8322
  Layer 5: LB min=-2.8529, max=15.5575 | UB min=2.6570, max=21.7716
  Layer 6: LB min=1.4811, max=7.2496 | UB min=9.2705, max=12.8116
  Layer 7: LB min=-1.3067, max=-1.3067 | UB min=5.7653, max=5.7653
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0033757686614990234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0010907649993896484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0017211437225341797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.01014089584350586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0017578601837158203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014066696166992188

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.5249, max=3.1248 | UB min=-3.1209, max=4.3456
  Layer 2: LB min=-3.5719, max=6.4310 | UB min=-1.6418, max=8.5677
  Layer 3: LB min=-7.6547, max=11.3143 | UB min=-2.3557, max=16.0953
  Layer 4: LB min=-2.8956, max=15.7226 | UB min=1.9586, max=21.0872
  Layer 5: LB min=-2.9157, max=15.7226 | UB min=2.7048, max=22.0439
  Layer 6: LB min=1.4456, max=7.3150 | UB min=9.3676, max=12.9544
  Layer 7: LB min=-1.3954, max=-1.3954 | UB min=5.8026, max=5.8026
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029663801193237305
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018339157104492188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.008016347885131836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.00997304916381836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0025467872619628906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0026776790618896484

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.5224, max=3.1286 | UB min=-3.1276, max=4.3414
  Layer 2: LB min=-3.5658, max=6.4386 | UB min=-1.6490, max=8.5607
  Layer 3: LB min=-7.6346, max=11.2521 | UB min=-2.3728, max=15.9991
  Layer 4: LB min=-2.8770, max=15.6400 | UB min=1.9391, max=20.9597
  Layer 5: LB min=-2.8843, max=15.6400 | UB min=2.6809, max=21.9078
  Layer 6: LB min=1.4634, max=7.2823 | UB min=9.3191, max=12.8830
  Layer 7: LB min=-1.3511, max=-1.3511 | UB min=5.7840, max=5.7840
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.01981329917907715
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.025271177291870117
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.015106916427612305
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.012526750564575195
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016341209411621094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016989707946777344

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.5199, max=3.1324 | UB min=-3.1344, max=4.3372
  Layer 2: LB min=-3.5597, max=6.4461 | UB min=-1.6561, max=8.5538
  Layer 3: LB min=-7.6145, max=11.1900 | UB min=-2.3898, max=15.9028
  Layer 4: LB min=-2.8583, max=15.5575 | UB min=1.9197, max=20.8322
  Layer 5: LB min=-2.8529, max=15.5575 | UB min=2.6570, max=21.7716
  Layer 6: LB min=1.4811, max=7.2496 | UB min=9.2705, max=12.8116
  Layer 7: LB min=-1.3067, max=-1.3067 | UB min=5.7653, max=5.7653
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02723383903503418
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.028752803802490234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0015990734100341797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012984275817871094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0015249252319335938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015642642974853516
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029642343521118164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.024952173233032227
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.008379459381103516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.021288394927978516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004706859588623047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015766620635986328
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027498722076416016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.011080026626586914
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.016970396041870117
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.013823270797729492
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0024061203002929688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015001296997070312
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0022449493408203125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.005995273590087891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002838611602783203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001251220703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001394033432006836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0020339488983154297
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.01731395721435547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03710317611694336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.010469913482666016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0019481182098388672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016894340515136719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0040149688720703125
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02439427375793457
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03329610824584961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.008359670639038086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0015010833740234375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001619100570678711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016086101531982422
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.002353191375732422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0009889602661132812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009691715240478516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001066446304321289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010373592376708984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012850761413574219
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.014166593551635742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01720571517944336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0022776126861572266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0016169548034667969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002496004104614258
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.01230311393737793
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0264129638671875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.017267942428588867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.028925180435180664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.005574226379394531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0017819404602050781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0018608570098876953
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027446985244750977
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021954774856567383
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.004877805709838867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.01893472671508789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00833439826965332
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0035648345947265625
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.032784461975097656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0263674259185791
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.004552364349365234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0017163753509521484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.003576993942260742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001982450485229492
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02509474754333496
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.05017352104187012
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.013971567153930664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.004268646240234375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0038068294525146484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004340171813964844
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0073659420013427734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.023640155792236328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.01089024543762207
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0014350414276123047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012755393981933594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.00133514404296875
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.01783299446105957
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.016962289810180664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009279251098632812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007717609405517578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009267330169677734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0021021366119384766
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.020368576049804688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.012684106826782227
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0046079158782958984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0054569244384765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0034427642822265625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016026496887207031
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Loading ONNX model...
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.032640695571899414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.00829768180847168
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.01373744010925293
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0033893585205078125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.003576040267944336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0050466060638427734

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-38.9271, max=0.0000 | UB min=0.0000, max=44.3264
  Layer 2: LB min=-64.0756, max=0.0000 | UB min=0.0000, max=87.9854
  Layer 3: LB min=-253.9437, max=231.4309 | UB min=151.6804, max=630.7798
  Layer 4: LB min=-224.7223, max=357.5443 | UB min=187.3296, max=881.9558
  Layer 5: LB min=-312.2195, max=376.2960 | UB min=327.1163, max=995.0199
  Layer 6: LB min=-146.5595, max=198.1575 | UB min=596.2072, max=620.0873
  Layer 7: LB min=-550.6562, max=-550.6562 | UB min=172.5272, max=172.5272
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02983713150024414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.036226749420166016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012547969818115234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001878976821899414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001506805419921875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011553764343261719

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-20.0745, max=0.0000 | UB min=0.0000, max=23.9499
  Layer 2: LB min=-34.0802, max=0.0000 | UB min=0.0000, max=47.1560
  Layer 3: LB min=-127.8604, max=133.9420 | UB min=82.0673, max=340.4579
  Layer 4: LB min=-118.5983, max=205.3129 | UB min=96.0222, max=477.2887
  Layer 5: LB min=-161.3160, max=215.2611 | UB min=170.4622, max=534.8062
  Layer 6: LB min=-74.7375, max=117.1677 | UB min=306.8842, max=334.5389
  Layer 7: LB min=-281.7338, max=-281.7338 | UB min=91.1219, max=91.1219
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.017481088638305664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01610279083251953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013141632080078125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010476112365722656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001234292984008789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0030426979064941406

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-10.3431, max=0.0000 | UB min=0.0000, max=14.9672
  Layer 2: LB min=-17.9593, max=0.0000 | UB min=0.0000, max=26.9164
  Layer 3: LB min=-67.7698, max=86.7421 | UB min=40.8125, max=188.4062
  Layer 4: LB min=-60.3153, max=136.4123 | UB min=43.0214, max=266.0306
  Layer 5: LB min=-80.1976, max=134.8546 | UB min=81.3764, max=292.9034
  Layer 6: LB min=-37.1106, max=83.8746 | UB min=148.0609, max=182.4785
  Layer 7: LB min=-133.1852, max=-133.1852 | UB min=47.4650, max=47.4650
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.018151283264160156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02676868438720703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0017743110656738281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0014002323150634766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011544227600097656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013501644134521484

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-5.4167, max=2.3182 | UB min=0.0000, max=10.8639
  Layer 2: LB min=-9.8572, max=1.1692 | UB min=0.0000, max=17.2643
  Layer 3: LB min=-31.7105, max=61.3396 | UB min=18.0621, max=102.2722
  Layer 4: LB min=-30.6478, max=94.3696 | UB min=14.8615, max=146.0233
  Layer 5: LB min=-31.9183, max=93.7345 | UB min=35.7315, max=156.3183
  Layer 6: LB min=-21.6586, max=60.1500 | UB min=58.7342, max=100.0097
  Layer 7: LB min=-50.3131, max=-50.3131 | UB min=18.9796, max=18.9796
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.020397663116455078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.028847932815551758
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009541511535644531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011060237884521484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0044918060302734375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011086463928222656

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.9112, max=4.3958 | UB min=0.0000, max=8.7672
  Layer 2: LB min=-6.4876, max=5.6709 | UB min=0.0000, max=13.5746
  Layer 3: LB min=-17.7006, max=37.4099 | UB min=5.5304, max=55.2746
  Layer 4: LB min=-18.5535, max=57.8266 | UB min=0.9262, max=79.2755
  Layer 5: LB min=-8.7860, max=59.7962 | UB min=13.3014, max=82.8645
  Layer 6: LB min=-10.0545, max=37.4099 | UB min=18.8562, max=54.1802
  Layer 7: LB min=-13.9888, max=-13.9888 | UB min=10.8935, max=10.8935
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.016749143600463867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021499156951904297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011317729949951172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010991096496582031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0055942535400390625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004754781723022461

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-1.6490, max=5.4420 | UB min=0.0000, max=7.7119
  Layer 2: LB min=-4.8928, max=7.9306 | UB min=-0.6411, max=11.9986
  Layer 3: LB min=-11.4368, max=20.3580 | UB min=-0.0225, max=27.6713
  Layer 4: LB min=-13.3334, max=32.2216 | UB min=-5.7944, max=40.8230
  Layer 5: LB min=-1.5675, max=35.8560 | UB min=5.7119, max=44.8578
  Layer 6: LB min=-5.1583, max=20.3580 | UB min=6.2287, max=27.6713
  Layer 7: LB min=-2.5983, max=-2.5983 | UB min=6.2385, max=6.2385
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.016832828521728516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.014075994491577148
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.003117084503173828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.007668972015380859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0035910606384277344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0024483203887939453

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-1.0140, max=5.9659 | UB min=0.0000, max=7.1810
  Layer 2: LB min=-4.0691, max=9.0625 | UB min=-1.8281, max=11.2064
  Layer 3: LB min=-8.6222, max=10.6371 | UB min=-2.7280, max=15.1485
  Layer 4: LB min=-11.8755, max=17.9531 | UB min=-8.2262, max=22.3491
  Layer 5: LB min=-0.1741, max=22.3585 | UB min=3.5326, max=27.0366
  Layer 6: LB min=-3.0551, max=9.7904 | UB min=2.7218, max=13.5675
  Layer 7: LB min=0.1107, max=0.1107 | UB min=4.1707, max=4.1707
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009627342224121094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0009672641754150391
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009362697601318359
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0009570121765136719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.000942230224609375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0018508434295654297

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-1.3319, max=5.7037 | UB min=0.0000, max=7.4474
  Layer 2: LB min=-4.4876, max=8.4967 | UB min=-1.2343, max=11.6149
  Layer 3: LB min=-10.0976, max=15.0633 | UB min=-1.3744, max=20.6115
  Layer 4: LB min=-12.7151, max=24.9534 | UB min=-7.1277, max=31.5436
  Layer 5: LB min=-0.8526, max=29.0457 | UB min=4.6215, max=35.8628
  Layer 6: LB min=-4.1779, max=15.0633 | UB min=4.4545, max=20.6115
  Layer 7: LB min=-1.2334, max=-1.2334 | UB min=5.2126, max=5.2126
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027630090713500977
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0211029052734375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012712478637695312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001058816909790039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010898113250732422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.011340141296386719

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-1.4906, max=5.5728 | UB min=0.0000, max=7.5796
  Layer 2: LB min=-4.6932, max=8.2135 | UB min=-0.9376, max=11.8084
  Layer 3: LB min=-10.7672, max=17.7193 | UB min=-0.6985, max=24.1500
  Layer 4: LB min=-13.0243, max=28.5772 | UB min=-6.5277, max=36.1843
  Layer 5: LB min=-1.1707, max=32.4515 | UB min=5.1665, max=40.3181
  Layer 6: LB min=-4.6681, max=17.7193 | UB min=5.3192, max=24.1500
  Layer 7: LB min=-1.8892, max=-1.8892 | UB min=5.7138, max=5.7138
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.021670103073120117
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.020991802215576172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002081632614135742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0017368793487548828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016210079193115234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015039443969726562

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-1.4112, max=5.6383 | UB min=0.0000, max=7.5135
  Layer 2: LB min=-4.5906, max=8.3551 | UB min=-1.0859, max=11.7133
  Layer 3: LB min=-10.4324, max=16.3913 | UB min=-1.0365, max=22.3807
  Layer 4: LB min=-12.8697, max=26.7581 | UB min=-6.8525, max=33.8639
  Layer 5: LB min=-1.0169, max=30.7486 | UB min=4.8939, max=38.0904
  Layer 6: LB min=-4.4230, max=16.3913 | UB min=4.8877, max=22.3807
  Layer 7: LB min=-1.5626, max=-1.5626 | UB min=5.4654, max=5.4654
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02225017547607422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.027220964431762695
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.013896465301513672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.004607677459716797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004762411117553711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0036208629608154297

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-1.3715, max=5.6710 | UB min=0.0000, max=7.4804
  Layer 2: LB min=-4.5391, max=8.4259 | UB min=-1.1601, max=11.6649
  Layer 3: LB min=-10.2650, max=15.7273 | UB min=-1.2055, max=21.4961
  Layer 4: LB min=-12.7924, max=25.8555 | UB min=-6.9901, max=32.7038
  Layer 5: LB min=-0.9349, max=29.8972 | UB min=4.7577, max=36.9766
  Layer 6: LB min=-4.3005, max=15.7273 | UB min=4.6711, max=21.4961
  Layer 7: LB min=-1.3979, max=-1.3979 | UB min=5.3393, max=5.3393
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.024073362350463867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03199434280395508
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0031430721282958984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0035533905029296875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.007692575454711914
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0025916099548339844

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-1.3914, max=5.6546 | UB min=0.0000, max=7.4970
  Layer 2: LB min=-4.5649, max=8.3905 | UB min=-1.1230, max=11.6896
  Layer 3: LB min=-10.3487, max=16.0593 | UB min=-1.1210, max=21.9384
  Layer 4: LB min=-12.8310, max=26.3067 | UB min=-6.9213, max=33.2839
  Layer 5: LB min=-0.9759, max=30.3229 | UB min=4.8258, max=37.5335
  Layer 6: LB min=-4.3618, max=16.0593 | UB min=4.7794, max=21.9384
  Layer 7: LB min=-1.4802, max=-1.4802 | UB min=5.4024, max=5.4024
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02794933319091797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.016221284866333008
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0015554428100585938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0017328262329101562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012662410736083984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002861499786376953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-1.3815, max=5.6628 | UB min=0.0000, max=7.4887
  Layer 2: LB min=-4.5520, max=8.4082 | UB min=-1.1416, max=11.6773
  Layer 3: LB min=-10.3069, max=15.8933 | UB min=-1.1632, max=21.7173
  Layer 4: LB min=-12.8117, max=26.0811 | UB min=-6.9557, max=32.9938
  Layer 5: LB min=-0.9554, max=30.1100 | UB min=4.7918, max=37.2551
  Layer 6: LB min=-4.3311, max=15.8933 | UB min=4.7252, max=21.7173
  Layer 7: LB min=-1.4390, max=-1.4390 | UB min=5.3709, max=5.3709
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.026612281799316406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.028515100479125977
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011851787567138672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.006048917770385742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013930797576904297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011625289916992188

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-1.3765, max=5.6669 | UB min=0.0000, max=7.4846
  Layer 2: LB min=-4.5456, max=8.4171 | UB min=-1.1508, max=11.6711
  Layer 3: LB min=-10.2859, max=15.8103 | UB min=-1.1844, max=21.6067
  Layer 4: LB min=-12.8021, max=25.9683 | UB min=-6.9729, max=32.8488
  Layer 5: LB min=-0.9452, max=30.0036 | UB min=4.7747, max=37.1159
  Layer 6: LB min=-4.3158, max=15.8103 | UB min=4.6982, max=21.6067
  Layer 7: LB min=-1.4185, max=-1.4185 | UB min=5.3551, max=5.3551
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0315394401550293
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.031358957290649414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019381046295166016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013720989227294922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011112689971923828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013332366943359375

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-1.3715, max=5.6710 | UB min=0.0000, max=7.4804
  Layer 2: LB min=-4.5391, max=8.4259 | UB min=-1.1601, max=11.6649
  Layer 3: LB min=-10.2650, max=15.7273 | UB min=-1.2055, max=21.4961
  Layer 4: LB min=-12.7924, max=25.8555 | UB min=-6.9901, max=32.7038
  Layer 5: LB min=-0.9349, max=29.8972 | UB min=4.7577, max=36.9766
  Layer 6: LB min=-4.3005, max=15.7273 | UB min=4.6711, max=21.4961
  Layer 7: LB min=-1.3979, max=-1.3979 | UB min=5.3393, max=5.3393
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029514789581298828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03415799140930176
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0017387866973876953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010678768157958984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013418197631835938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013325214385986328
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0037262439727783203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0012354850769042969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008702278137207031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012700557708740234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013453960418701172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010247230529785156
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0017206668853759766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0007557868957519531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007524490356445312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0060040950775146484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001451730728149414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013911724090576172
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.025777578353881836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01741814613342285
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012466907501220703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001552581787109375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013613700866699219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013272762298583984
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.028876066207885742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01434183120727539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013837814331054688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011844635009765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001119852066040039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011110305786132812
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0033965110778808594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0011494159698486328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001470327377319336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0016443729400634766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0018992424011230469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016248226165771484
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.031169891357421875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.028357744216918945
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001382589340209961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010263919830322266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013132095336914062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0017328262329101562
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.015677690505981445
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.008220911026000977
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002134084701538086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0036835670471191406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001859903335571289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0017633438110351562
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.019580602645874023
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.011819839477539062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.00139617919921875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010979175567626953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010874271392822266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013763904571533203
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027418136596679688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.032799720764160156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013113021850585938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010266304016113281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012006759643554688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0017795562744140625
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027942180633544922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03374361991882324
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.00563812255859375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0014607906341552734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012636184692382812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012788772583007812
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.017636537551879883
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.030620098114013672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012826919555664062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010480880737304688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0051157474517822266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014219284057617188
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.021471500396728516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.035125732421875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0023107528686523438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.004224061965942383
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0017609596252441406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014910697937011719
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.020917892456054688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.014354228973388672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011477470397949219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012602806091308594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001840353012084961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0027289390563964844
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.01809096336364746
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.022259950637817383
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012085437774658203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008051395416259766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009403228759765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010061264038085938
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Loading ONNX model...
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.025926828384399414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.016376733779907227
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0036079883575439453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013091564178466797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013091564178466797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012233257293701172

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-39.1275, max=0.0000 | UB min=0.0000, max=54.0211
  Layer 2: LB min=-60.2343, max=0.0000 | UB min=0.0000, max=90.2224
  Layer 3: LB min=-254.1095, max=220.9231 | UB min=146.0222, max=614.4603
  Layer 4: LB min=-219.2252, max=342.6267 | UB min=183.9126, max=859.2984
  Layer 5: LB min=-307.2299, max=361.1032 | UB min=320.2803, max=972.1756
  Layer 6: LB min=-146.4383, max=188.6563 | UB min=586.2902, max=603.9069
  Layer 7: LB min=-544.0869, max=-544.0869 | UB min=169.3641, max=169.3641
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.032752037048339844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.026673078536987305
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012059211730957031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0026769638061523438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012102127075195312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012750625610351562

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-20.6123, max=0.0000 | UB min=0.0000, max=28.8692
  Layer 2: LB min=-32.6116, max=0.0000 | UB min=0.0000, max=45.2651
  Layer 3: LB min=-130.0739, max=118.8478 | UB min=74.4700, max=321.2626
  Layer 4: LB min=-112.1307, max=184.2682 | UB min=93.5690, max=450.2776
  Layer 5: LB min=-157.3934, max=194.1157 | UB min=163.4669, max=508.2423
  Layer 6: LB min=-75.2740, max=103.8595 | UB min=298.0744, max=315.0767
  Layer 7: LB min=-278.0092, max=-278.0092 | UB min=88.0570, max=88.0570
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.031510114669799805
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02059173583984375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011830329895019531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010342597961425781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016002655029296875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002236604690551758

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-11.2038, max=0.0000 | UB min=0.0000, max=16.1094
  Layer 2: LB min=-18.4664, max=0.0000 | UB min=0.0000, max=23.6685
  Layer 3: LB min=-65.3646, max=67.3428 | UB min=36.9007, max=169.7294
  Layer 4: LB min=-56.1906, max=103.9934 | UB min=46.2630, max=238.2537
  Layer 5: LB min=-76.8331, max=109.0037 | UB min=81.7069, max=265.9713
  Layer 6: LB min=-38.2725, max=61.5683 | UB min=145.4623, max=166.0840
  Layer 7: LB min=-137.0361, max=-137.0361 | UB min=45.6148, max=45.6148
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030494213104248047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03222227096557617
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013546943664550781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002530336380004883
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0015537738800048828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0022165775299072266

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-6.4501, max=1.1122 | UB min=0.0000, max=9.9695
  Layer 2: LB min=-10.9605, max=0.0000 | UB min=0.0000, max=13.7584
  Layer 3: LB min=-32.3885, max=40.7499 | UB min=15.4159, max=87.4404
  Layer 4: LB min=-26.1162, max=66.3615 | UB min=20.3216, max=124.2919
  Layer 5: LB min=-33.0458, max=66.1950 | UB min=36.6535, max=136.4416
  Layer 6: LB min=-16.0542, max=40.1264 | UB min=61.6257, max=83.6865
  Layer 7: LB min=-57.1451, max=-57.1451 | UB min=23.1010, max=23.1010
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.017644882202148438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.004162788391113281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002092123031616211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002365589141845703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0033309459686279297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.006232261657714844

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-4.0660, max=3.1273 | UB min=0.0000, max=7.6403
  Layer 2: LB min=-7.7363, max=3.0145 | UB min=-0.9269, max=9.7055
  Layer 3: LB min=-17.9257, max=24.8912 | UB min=1.7583, max=43.9794
  Layer 4: LB min=-10.4755, max=40.3093 | UB min=8.1412, max=61.3628
  Layer 5: LB min=-10.6910, max=39.2952 | UB min=14.4375, max=67.3533
  Layer 6: LB min=-8.4027, max=24.8912 | UB min=20.5714, max=41.8356
  Layer 7: LB min=-19.1225, max=-19.1225 | UB min=12.5052, max=12.5052
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.021190404891967773
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018549203872680664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001401662826538086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0016417503356933594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013575553894042969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013353824615478516

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.8708, max=4.1375 | UB min=-0.6212, max=6.4703
  Layer 2: LB min=-6.2939, max=4.8687 | UB min=-3.0853, max=7.9185
  Layer 3: LB min=-12.9145, max=14.6804 | UB min=-3.6923, max=23.5916
  Layer 4: LB min=-3.8365, max=23.9052 | UB min=3.9314, max=32.8378
  Layer 5: LB min=-2.6836, max=23.2061 | UB min=6.7492, max=35.8897
  Layer 6: LB min=-6.2403, max=14.6804 | UB min=5.5990, max=21.5102
  Layer 7: LB min=-5.1374, max=-5.1374 | UB min=5.6015, max=5.6015
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.01342916488647461
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.006623268127441406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002616405487060547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002249002456665039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0038683414459228516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002605915069580078

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.2723, max=4.6431 | UB min=-1.0745, max=5.8840
  Layer 2: LB min=-5.5930, max=5.5753 | UB min=-3.9907, max=7.0747
  Layer 3: LB min=-10.5912, max=7.9108 | UB min=-5.9750, max=12.5380
  Layer 4: LB min=-1.2225, max=16.9554 | UB min=1.7994, max=21.2182
  Layer 5: LB min=-0.0163, max=16.4241 | UB min=4.0359, max=22.6786
  Layer 6: LB min=-4.3321, max=7.9108 | UB min=0.0945, max=10.9282
  Layer 7: LB min=0.4816, max=0.4816 | UB min=3.5563, max=3.5563
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0020585060119628906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0029768943786621094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.003595590591430664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002497434616088867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002470731735229492
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.006100177764892578

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.5719, max=4.3901 | UB min=-0.8475, max=6.1775
  Layer 2: LB min=-5.9438, max=5.2831 | UB min=-3.5822, max=7.4877
  Layer 3: LB min=-11.7336, max=11.3492 | UB min=-4.9032, max=17.8405
  Layer 4: LB min=-2.4228, max=20.3948 | UB min=2.8154, max=26.8607
  Layer 5: LB min=-1.3047, max=19.7624 | UB min=5.2525, max=29.0912
  Layer 6: LB min=-5.2689, max=11.3492 | UB min=2.5397, max=15.9704
  Layer 7: LB min=-2.0970, max=-2.0970 | UB min=4.2899, max=4.2899
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027568817138671875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015630722045898438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012924671173095703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011951923370361328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012760162353515625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011513233184814453

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.4222, max=4.5165 | UB min=-0.9609, max=6.0308
  Layer 2: LB min=-5.7685, max=5.4290 | UB min=-3.7979, max=7.2757
  Layer 3: LB min=-11.1525, max=9.7036 | UB min=-5.4432, max=15.2000
  Layer 4: LB min=-1.7900, max=18.7296 | UB min=2.2824, max=24.0548
  Layer 5: LB min=-0.6427, max=18.1292 | UB min=4.6188, max=25.8897
  Layer 6: LB min=-4.7944, max=9.7036 | UB min=1.2526, max=13.4630
  Layer 7: LB min=-0.7301, max=-0.7301 | UB min=3.8676, max=3.8676
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.003005504608154297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0039713382720947266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002210855484008789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0023889541625976562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0047779083251953125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.011129140853881836

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.4972, max=4.4532 | UB min=-0.9041, max=6.1043
  Layer 2: LB min=-5.8562, max=5.3559 | UB min=-3.6928, max=7.3805
  Layer 3: LB min=-11.4410, max=10.5487 | UB min=-5.1741, max=16.5227
  Layer 4: LB min=-2.0979, max=19.5744 | UB min=2.5418, max=25.4590
  Layer 5: LB min=-0.9693, max=18.9535 | UB min=4.9289, max=27.4888
  Layer 6: LB min=-5.0305, max=10.5487 | UB min=1.8775, max=14.7205
  Layer 7: LB min=-1.3977, max=-1.3977 | UB min=4.0613, max=4.0613
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0010235309600830078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006654262542724609
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006611347198486328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006403923034667969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008890628814697266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002780914306640625

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.4597, max=4.4849 | UB min=-0.9325, max=6.0676
  Layer 2: LB min=-5.8123, max=5.3925 | UB min=-3.7480, max=7.3268
  Layer 3: LB min=-11.2944, max=10.1468 | UB min=-5.3096, max=15.8628
  Layer 4: LB min=-1.9363, max=19.1632 | UB min=2.4052, max=24.7572
  Layer 5: LB min=-0.8012, max=18.5492 | UB min=4.7673, max=26.6863
  Layer 6: LB min=-4.9113, max=10.1468 | UB min=1.5466, max=14.0947
  Layer 7: LB min=-1.0499, max=-1.0499 | UB min=3.9477, max=3.9477
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.015156269073486328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015941619873046875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008933544158935547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008099079132080078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007889270782470703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010290145874023438

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.4410, max=4.5007 | UB min=-0.9467, max=6.0492
  Layer 2: LB min=-5.7904, max=5.4107 | UB min=-3.7738, max=7.3008
  Layer 3: LB min=-11.2227, max=9.9277 | UB min=-5.3766, max=15.5319
  Layer 4: LB min=-1.8614, max=18.9480 | UB min=2.3428, max=24.4060
  Layer 5: LB min=-0.7211, max=18.3402 | UB min=4.6918, max=26.2878
  Layer 6: LB min=-4.8525, max=9.9277 | UB min=1.3976, max=13.7798
  Layer 7: LB min=-0.8878, max=-0.8878 | UB min=3.9065, max=3.9065
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0009062290191650391
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0006003379821777344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006239414215087891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007612705230712891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007646083831787109
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0034780502319335938

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.4503, max=4.4928 | UB min=-0.9396, max=6.0584
  Layer 2: LB min=-5.8014, max=5.4016 | UB min=-3.7617, max=7.3134
  Layer 3: LB min=-11.2578, max=10.0397 | UB min=-5.3433, max=15.6978
  Layer 4: LB min=-1.8973, max=19.0572 | UB min=2.3731, max=24.5817
  Layer 5: LB min=-0.7604, max=18.4458 | UB min=4.7283, max=26.4868
  Layer 6: LB min=-4.8815, max=10.0397 | UB min=1.4702, max=13.9382
  Layer 7: LB min=-0.9671, max=-0.9671 | UB min=3.9260, max=3.9260
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0008540153503417969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0005688667297363281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0006167888641357422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0006654262542724609
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007185935974121094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0008821487426757812

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.4550, max=4.4888 | UB min=-0.9360, max=6.0630
  Layer 2: LB min=-5.8069, max=5.3970 | UB min=-3.7548, max=7.3201
  Layer 3: LB min=-11.2761, max=10.0932 | UB min=-5.3265, max=15.7803
  Layer 4: LB min=-1.9168, max=19.1102 | UB min=2.3892, max=24.6694
  Layer 5: LB min=-0.7808, max=18.4975 | UB min=4.7478, max=26.5866
  Layer 6: LB min=-4.8964, max=10.0932 | UB min=1.5084, max=14.0164
  Layer 7: LB min=-1.0085, max=-1.0085 | UB min=3.9368, max=3.9368
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.016871929168701172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.023636817932128906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009789466857910156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0024962425231933594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0015609264373779297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012967586517333984

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-2.4550, max=4.4888 | UB min=-0.9360, max=6.0630
  Layer 2: LB min=-5.8069, max=5.3970 | UB min=-3.7548, max=7.3201
  Layer 3: LB min=-11.2761, max=10.0932 | UB min=-5.3265, max=15.7803
  Layer 4: LB min=-1.9168, max=19.1102 | UB min=2.3892, max=24.6694
  Layer 5: LB min=-0.7808, max=18.4975 | UB min=4.7478, max=26.5866
  Layer 6: LB min=-4.8964, max=10.0932 | UB min=1.5084, max=14.0164
  Layer 7: LB min=-1.0085, max=-1.0085 | UB min=3.9368, max=3.9368
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.008685111999511719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.008197307586669922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0028820037841796875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010159015655517578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010020732879638672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009272098541259766
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.022765159606933594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01609659194946289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.005371809005737305
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002375364303588867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013651847839355469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012462139129638672
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027728796005249023
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.031142234802246094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013022422790527344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.003389120101928711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013551712036132812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013544559478759766
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02408766746520996
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.014502525329589844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014159679412841797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0015223026275634766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001287221908569336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0026040077209472656
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02331709861755371
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018752098083496094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008943080902099609
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0024611949920654297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008943080902099609
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010912418365478516
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.048627614974975586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018041372299194336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016112327575683594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001252889633178711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001146078109741211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013012886047363281
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0014336109161376953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.009606122970581055
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0033838748931884766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0036444664001464844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00350189208984375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014729499816894531
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.002323150634765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.008223533630371094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0042629241943359375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0019109249114990234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001489400863647461
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013225078582763672
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.042433977127075195
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018761157989501953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0026547908782958984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.003076791763305664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.012332677841186523
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0041468143463134766
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.05204057693481445
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02073383331298828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.008016347885131836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002196788787841797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002834320068359375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013647079467773438
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03425025939941406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02258467674255371
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014770030975341797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.005401134490966797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013408660888671875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0032057762145996094
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.034982919692993164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03266191482543945
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016906261444091797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012288093566894531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.005062103271484375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014190673828125
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.04689311981201172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01833510398864746
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012679100036621094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001058340072631836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012593269348144531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001245260238647461
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030051231384277344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01585984230041504
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012271404266357422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011403560638427734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0015103816986083984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016236305236816406
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02372288703918457
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03061962127685547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014448165893554688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012443065643310547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00793147087097168
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013954639434814453
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Loading ONNX model...
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.025205135345458984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.026590824127197266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0031495094299316406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011851787567138672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011637210845947266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0025131702423095703

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-41.4154, max=0.0000 | UB min=0.0000, max=56.5887
  Layer 2: LB min=-61.7117, max=0.0000 | UB min=0.0000, max=89.2948
  Layer 3: LB min=-257.5624, max=233.2904 | UB min=145.4929, max=627.1477
  Layer 4: LB min=-223.0384, max=361.9806 | UB min=187.0624, max=882.7747
  Layer 5: LB min=-310.2170, max=381.0818 | UB min=325.1184, max=996.6683
  Layer 6: LB min=-145.3326, max=199.5324 | UB min=592.8398, max=618.7193
  Layer 7: LB min=-546.5927, max=-546.5927 | UB min=172.1947, max=172.1947
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.016421794891357422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.029857158660888672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0029146671295166016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0035390853881835938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004417896270751953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0031578540802001953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-23.8025, max=0.0000 | UB min=0.0000, max=32.2929
  Layer 2: LB min=-33.1006, max=0.0000 | UB min=0.0000, max=49.1834
  Layer 3: LB min=-131.6811, max=137.0198 | UB min=71.0584, max=332.6687
  Layer 4: LB min=-114.7294, max=211.4131 | UB min=94.7498, max=474.5311
  Layer 5: LB min=-154.5321, max=221.7957 | UB min=165.2044, max=530.8118
  Layer 6: LB min=-71.5991, max=120.5465 | UB min=295.5825, max=330.3440
  Layer 7: LB min=-270.2395, max=-270.2395 | UB min=89.4442, max=89.4442
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.009219646453857422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.002457141876220703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002732992172241211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0031647682189941406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0051937103271484375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002828359603881836

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-14.8550, max=1.0571 | UB min=0.0000, max=19.8511
  Layer 2: LB min=-17.8628, max=1.3752 | UB min=0.0000, max=28.4174
  Layer 3: LB min=-64.2780, max=86.9637 | UB min=29.2493, max=176.5474
  Layer 4: LB min=-56.6895, max=135.8564 | UB min=43.8470, max=256.5627
  Layer 5: LB min=-68.7904, max=140.9653 | UB min=76.8028, max=282.0787
  Layer 6: LB min=-33.5549, max=81.3268 | UB min=131.8122, max=176.5474
  Layer 7: LB min=-117.3057, max=-117.3057 | UB min=45.4398, max=45.4398
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03133273124694824
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.028229236602783203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008003711700439453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008130073547363281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0039408206939697266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014736652374267578

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-10.2847, max=5.3486 | UB min=-1.5019, max=13.9261
  Layer 2: LB min=-10.7413, max=7.1583 | UB min=0.0000, max=19.8831
  Layer 3: LB min=-35.3358, max=49.2909 | UB min=7.7434, max=88.3114
  Layer 4: LB min=-26.4783, max=85.7099 | UB min=18.7602, max=132.8495
  Layer 5: LB min=-25.6548, max=87.1266 | UB min=32.9102, max=146.1478
  Layer 6: LB min=-15.3617, max=49.2909 | UB min=50.5913, max=88.3114
  Layer 7: LB min=-40.6929, max=-40.6929 | UB min=26.5776, max=26.5776
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.04734539985656738
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02236032485961914
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013582706451416016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.008544921875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012989044189453125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0017883777618408203

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-7.9899, max=7.5098 | UB min=-3.4622, max=11.9231
  Layer 2: LB min=-7.8382, max=9.8293 | UB min=-1.2239, max=16.2776
  Layer 3: LB min=-25.1159, max=26.5304 | UB min=-5.1010, max=43.8882
  Layer 4: LB min=-11.6259, max=51.8115 | UB min=8.4971, max=68.3142
  Layer 5: LB min=-8.0032, max=50.1864 | UB min=15.7047, max=75.6959
  Layer 6: LB min=-9.7657, max=26.5304 | UB min=18.1918, max=43.8882
  Layer 7: LB min=-9.5562, max=-9.5562 | UB min=18.2576, max=18.2576
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03341817855834961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.013117074966430664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0029382705688476562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010755062103271484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001722097396850586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012829303741455078

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-9.1373, max=6.4285 | UB min=-2.4799, max=12.9254
  Layer 2: LB min=-9.2921, max=8.4913 | UB min=0.0000, max=18.0935
  Layer 3: LB min=-30.0150, max=38.4320 | UB min=1.3790, max=66.2087
  Layer 4: LB min=-18.6547, max=69.4133 | UB min=12.8798, max=99.0792
  Layer 5: LB min=-15.8018, max=69.2740 | UB min=22.9450, max=110.1150
  Layer 6: LB min=-12.5479, max=38.4320 | UB min=32.6642, max=66.2087
  Layer 7: LB min=-23.6668, max=-23.6668 | UB min=22.2771, max=22.2771
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03247475624084473
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.019602060317993164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011835098266601562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001104593276977539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0025315284729003906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012655258178710938

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.5636, max=6.9691 | UB min=-2.9691, max=12.4249
  Layer 2: LB min=-8.5664, max=9.1588 | UB min=-0.3045, max=17.1878
  Layer 3: LB min=-27.4299, max=32.5832 | UB min=-1.9012, max=54.9629
  Layer 4: LB min=-15.0135, max=62.0030 | UB min=10.3233, max=83.0790
  Layer 5: LB min=-11.4913, max=59.9107 | UB min=18.8732, max=92.5205
  Layer 6: LB min=-11.1238, max=32.5832 | UB min=24.8022, max=54.9629
  Layer 7: LB min=-15.9279, max=-15.9279 | UB min=20.1223, max=20.1223
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029813766479492188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.026613712310791016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011448860168457031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0009531974792480469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0039141178131103516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015828609466552734

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.8504, max=6.6988 | UB min=-2.7244, max=12.6752
  Layer 2: LB min=-8.9298, max=8.8248 | UB min=0.0000, max=17.6429
  Layer 3: LB min=-28.6490, max=35.5718 | UB min=-0.2998, max=60.5424
  Layer 4: LB min=-16.7660, max=67.0573 | UB min=11.3484, max=90.5396
  Layer 5: LB min=-13.3084, max=64.7048 | UB min=20.5746, max=100.9696
  Layer 6: LB min=-11.8221, max=35.5718 | UB min=28.2455, max=60.5424
  Layer 7: LB min=-19.3066, max=-19.3066 | UB min=21.0737, max=21.0737
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.026896953582763672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.023561477661132812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018703937530517578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011856555938720703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004566669464111328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.00153350830078125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9939, max=6.5636 | UB min=-2.6021, max=12.8003
  Layer 2: LB min=-9.1110, max=8.6580 | UB min=0.0000, max=17.8696
  Layer 3: LB min=-29.3815, max=36.9815 | UB min=0.5833, max=63.4197
  Layer 4: LB min=-17.7344, max=67.4100 | UB min=12.1390, max=94.8803
  Layer 5: LB min=-14.5802, max=67.0400 | UB min=21.7355, max=105.6487
  Layer 6: LB min=-12.2002, max=36.9815 | UB min=30.5410, max=63.4197
  Layer 7: LB min=-21.6156, max=-21.6156 | UB min=21.7100, max=21.7100
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03592038154602051
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01721024513244629
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016961097717285156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012440681457519531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010764598846435547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0030400753021240234

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9221, max=6.6312 | UB min=-2.6633, max=12.7377
  Layer 2: LB min=-9.0204, max=8.7414 | UB min=0.0000, max=17.7577
  Layer 3: LB min=-29.0649, max=36.2586 | UB min=0.1855, max=62.0253
  Layer 4: LB min=-17.2745, max=66.4216 | UB min=11.7720, max=92.7893
  Layer 5: LB min=-13.9692, max=65.9257 | UB min=21.1353, max=103.4159
  Layer 6: LB min=-12.0263, max=36.2586 | UB min=29.4838, max=62.0253
  Layer 7: LB min=-20.5934, max=-20.5934 | UB min=21.4186, max=21.4186
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.01959371566772461
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.020338773727416992
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010809898376464844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0018308162689208984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012426376342773438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.00139617919921875

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.8863, max=6.6650 | UB min=-2.6938, max=12.7064
  Layer 2: LB min=-8.9751, max=8.7831 | UB min=0.0000, max=17.6998
  Layer 3: LB min=-28.8077, max=35.9314 | UB min=-0.0999, max=61.2395
  Layer 4: LB min=-16.9959, max=67.6808 | UB min=11.5248, max=91.4808
  Layer 5: LB min=-13.5610, max=65.2968 | UB min=20.8294, max=102.0358
  Layer 6: LB min=-11.9092, max=35.9314 | UB min=28.7253, max=61.2395
  Layer 7: LB min=-19.7743, max=-19.7743 | UB min=21.2125, max=21.2125
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0274355411529541
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0327913761138916
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016965866088867188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0017747879028320312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001186370849609375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013034343719482422

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03891944885253906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02687358856201172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013213157653808594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011734962463378906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001687765121459961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012049674987792969

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9132, max=6.6397 | UB min=-2.6709, max=12.7299
  Layer 2: LB min=-9.0090, max=8.7518 | UB min=0.0000, max=17.7437
  Layer 3: LB min=-29.0252, max=36.1683 | UB min=0.1358, max=61.8509
  Layer 4: LB min=-17.2169, max=66.2987 | UB min=11.7275, max=92.5305
  Layer 5: LB min=-13.8933, max=65.7865 | UB min=21.0613, max=103.1369
  Layer 6: LB min=-12.0046, max=36.1683 | UB min=29.3524, max=61.8509
  Layer 7: LB min=-20.4663, max=-20.4663 | UB min=21.3818, max=21.3818
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.028716564178466797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018935203552246094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011472702026367188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010211467742919922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011262893676757812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015125274658203125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9087, max=6.6439 | UB min=-2.6747, max=12.7260
  Layer 2: LB min=-9.0034, max=8.7570 | UB min=0.0000, max=17.7367
  Layer 3: LB min=-29.0055, max=36.1231 | UB min=0.1109, max=61.7637
  Layer 4: LB min=-17.1882, max=66.2373 | UB min=11.7053, max=92.4010
  Layer 5: LB min=-13.8554, max=65.7169 | UB min=21.0242, max=102.9973
  Layer 6: LB min=-11.9937, max=36.1231 | UB min=29.2867, max=61.7637
  Layer 7: LB min=-20.4028, max=-20.4028 | UB min=21.3634, max=21.3634
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027347564697265625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.019901752471923828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008985996246337891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.000972747802734375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001064300537109375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010275840759277344

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02231001853942871
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.016026973724365234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010037422180175781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013914108276367188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013306140899658203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002161741256713867
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.028445720672607422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.014761686325073242
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011687278747558594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012021064758300781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001150369644165039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012044906616210938
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.017235755920410156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01008749008178711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.003488302230834961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010883808135986328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011777877807617188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010766983032226562
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03674817085266113
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03238964080810547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013554096221923828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001028299331665039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.005826473236083984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0019228458404541016
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02766132354736328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021128416061401367
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011277198791503906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011830329895019531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004621744155883789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013153553009033203
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.05425381660461426
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018268585205078125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011525154113769531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001664876937866211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0015575885772705078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0021996498107910156
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Early stopping
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0068302154541015625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015137910842895508
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001486063003540039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0035605430603027344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0018830299377441406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015497207641601562
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03481888771057129
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.025709152221679688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010628700256347656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010657310485839844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0036020278930664062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010640621185302734
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03393721580505371
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03675079345703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013659000396728516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010595321655273438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.003212451934814453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002801179885864258
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03134727478027344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01574540138244629
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011620521545410156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010628700256347656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0013039112091064453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0035681724548339844
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.036418914794921875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.030538320541381836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001325368881225586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013720989227294922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001481771469116211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014243125915527344
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.05118513107299805
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.031870365142822266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0017087459564208984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001186370849609375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004620075225830078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0020673274993896484
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.028983354568481445
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.012236356735229492
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001354217529296875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010221004486083984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011620521545410156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.003625631332397461
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.037041664123535156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.028174161911010742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011124610900878906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0029675960540771484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0021986961364746094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012581348419189453
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029710769653320312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0273892879486084
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008654594421386719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010728836059570312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0007827281951904297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0027196407318115234
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
 Found input where NAP improves robustness! at index 4 epsilons are  0.044657470703125 and  0.030694396972656246
[INFO] Found NAP-exclusive robust input for label 7 at epsilon=0.0447
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.011421442031860352
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.013167858123779297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002303600311279297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002474069595336914
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.006742954254150391
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0018579959869384766

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 2
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 6
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.04166436195373535
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02127361297607422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012826919555664062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011246204376220703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011279582977294922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004287242889404297

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 0 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.025959253311157227
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03181266784667969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012290477752685547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011446475982666016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0021598339080810547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014986991882324219

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 1
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 5
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.003710508346557617
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0011944770812988281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013506412506103516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0016765594482421875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0033483505249023438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002081155776977539

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 0
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 5
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 9
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 2
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 2 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.020505905151367188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.019838809967041016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.004719257354736328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0032515525817871094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004216670989990234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.007207155227661133

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 3 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027785062789916992
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.006697654724121094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011293888092041016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010585784912109375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0054416656494140625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.006266117095947266

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 4 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0259091854095459
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.013132572174072266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014758110046386719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0018873214721679688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010421276092529297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009539127349853516

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03238964080810547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.05151844024658203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018129348754882812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001474618911743164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00961613655090332
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0032868385314941406

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 7
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 9
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030719995498657227
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021965742111206055
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.007189035415649414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.004079103469848633
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0020062923431396484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0018210411071777344

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 7 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0239565372467041
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.00740814208984375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011973381042480469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.004626750946044922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004635810852050781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004387617111206055

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029941320419311523
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021341800689697266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001497030258178711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013556480407714844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0015869140625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015916824340820312

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 4
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030200481414794922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02500772476196289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016145706176757812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001538991928100586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0023870468139648438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010416507720947266

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 8
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 10 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02835249900817871
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02722311019897461
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0019779205322265625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.003584146499633789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010492801666259766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009603500366210938

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 11 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03110790252685547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02214837074279785
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009529590606689453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008828639984130859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.012702465057373047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001775979995727539

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 12 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02579045295715332
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.028329133987426758
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002997159957885742
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008950233459472656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009331703186035156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.000995635986328125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0308685302734375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021458148956298828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001491546630859375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0007314682006835938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.003323078155517578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.000972747802734375

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 0
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 6
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03134942054748535
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03433394432067871
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011630058288574219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0009255409240722656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00506901741027832
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0020258426666259766

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 15 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03034806251525879
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.005814075469970703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.002134084701538086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.007577657699584961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001325845718383789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014405250549316406

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02799201011657715
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01969432830810547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016627311706542969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0014989376068115234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.006857395172119141
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013670921325683594

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 17 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030939340591430664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.017008304595947266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008769035339355469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0009329319000244141
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009160041809082031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0020601749420166016

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 7
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029666423797607422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.017520904541015625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.008647441864013672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0035278797149658203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002265453338623047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012483596801757812

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 19 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.021174192428588867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.009047269821166992
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009770393371582031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008881092071533203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010297298431396484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010428428649902344

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 20 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029859542846679688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0302579402923584
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013604164123535156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010387897491455078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010569095611572266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001026153564453125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 21 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02434253692626953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02226710319519043
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012328624725341797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008554458618164062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010311603546142578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.005797863006591797

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02837538719177246
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.029803752899169922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001127004623413086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011568069458007812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009644031524658203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.003859281539916992

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 23 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.031774282455444336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021229267120361328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009207725524902344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008871555328369141
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002734661102294922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0010924339294433594

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 24 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0251619815826416
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.009738922119140625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010771751403808594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0009145736694335938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008871555328369141
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004980564117431641

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 25 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02296757698059082
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.019791603088378906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010843276977539062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0021288394927978516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011661052703857422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012559890747070312

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02617812156677246
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01763772964477539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0017976760864257812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011582374572753906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011708736419677734
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012423992156982422

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.021292924880981445
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02359771728515625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012993812561035156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012121200561523438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002518892288208008
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014946460723876953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029863834381103516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03165459632873535
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012028217315673828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010995864868164062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001920938491821289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0018491744995117188

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02934741973876953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02530074119567871
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014641284942626953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0049381256103515625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0021889209747314453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001957416534423828

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0313105583190918
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021636009216308594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0036542415618896484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.004002571105957031
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.006833553314208984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0021495819091796875

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03344249725341797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.006799221038818359
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010609626770019531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001191854476928711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012018680572509766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001135110855102539

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 32 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.031830787658691406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.012268543243408203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0007698535919189453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012178421020507812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0014147758483886719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004067182540893555

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 33 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02412581443786621
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.026122331619262695
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008113384246826172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008628368377685547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009250640869140625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009529590606689453

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.026857852935791016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.029758214950561523
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016646385192871094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0014073848724365234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012745857238769531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012624263763427734

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 8
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.037869930267333984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018265962600708008
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001264810562133789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012164115905761719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012996196746826172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0025224685668945312

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 36 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.021381855010986328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015727519989013672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0009744167327880859
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010497570037841797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.005255937576293945
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011284351348876953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03272509574890137
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01910710334777832
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001360177993774414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001001596450805664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012097358703613281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.005476236343383789

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 38 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0323185920715332
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.012076616287231445
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011370182037353516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002117156982421875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012409687042236328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.007169485092163086

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
    [ACTive neuron in Nap] Layer 1, Neuron 3
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.022350311279296875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018386125564575195
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001573324203491211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010378360748291016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001216888427734375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012047290802001953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 40 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0279080867767334
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015761852264404297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014081001281738281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011203289031982422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0010733604431152344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012316703796386719

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 41 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.031219005584716797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02212214469909668
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.003892660140991211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012464523315429688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012886524200439453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0036764144897460938

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 42 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.025070667266845703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021410465240478516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012738704681396484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0068950653076171875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001383066177368164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001312255859375

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0396115779876709
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02084636688232422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011858940124511719
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011148452758789062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012021064758300781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0011255741119384766

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 44 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030876636505126953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01794147491455078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012099742889404297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001451253890991211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016024112701416016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014638900756835938

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.04172062873840332
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0299532413482666
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010988712310791016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0016372203826904297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.003187417984008789
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014584064483642578

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030478715896606445
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.027870893478393555
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0020220279693603516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013828277587890625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0017676353454589844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.00160980224609375

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03334641456604004
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.028522491455078125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011265277862548828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0045528411865234375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001373291015625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015888214111328125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03362417221069336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.026995182037353516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0021681785583496094
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011484622955322266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004884004592895508
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002680063247680664

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.022535085678100586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.024822473526000977
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014328956604003906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001425027847290039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001619577407836914
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016186237335205078

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.028228759765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.016689300537109375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016105175018310547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0014562606811523438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.006444692611694336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012011528015136719

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 51 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02838587760925293
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02237701416015625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014693737030029297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0017032623291015625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001661062240600586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0017676353454589844

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030879497528076172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.017971277236938477
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.003950595855712891
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0027251243591308594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004932403564453125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0019347667694091797

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 53 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.024030685424804688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.028255224227905273
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0030875205993652344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.003314495086669922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0044133663177490234
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004598140716552734

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030193328857421875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03535103797912598
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001218557357788086
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012340545654296875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002211332321166992
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014281272888183594

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.04444289207458496
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.036878108978271484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012140274047851562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011627674102783203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012276172637939453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014481544494628906

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 56 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02986454963684082
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.022537946701049805
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011620521545410156
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0015497207641601562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0014488697052001953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015196800231933594

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029292583465576172
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.019623756408691406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0028891563415527344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0032041072845458984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0033037662506103516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.011586666107177734

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.04161953926086426
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.023609161376953125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012311935424804688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013163089752197266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011172294616699219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0043108463287353516

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 59 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030191659927368164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015943527221679688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0034716129302978516
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012617111206054688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001191854476928711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.002531290054321289

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03172755241394043
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02112579345703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0037915706634521484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001256704330444336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00388336181640625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013480186462402344

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 61 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02991318702697754
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01898503303527832
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001165628433227539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010294914245605469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.011360883712768555
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0033631324768066406

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 62 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02985405921936035
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.019719362258911133
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.008730888366699219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010800361633300781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0030357837677001953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013110637664794922

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.05139613151550293
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.023535966873168945
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0018353462219238281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0008516311645507812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0008594989776611328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.003215312957763672

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 64 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.026580333709716797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.04092907905578613
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001791238784790039
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013391971588134766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0024094581604003906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015230178833007812

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.028594970703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01880955696105957
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011835098266601562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001016378402709961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012502670288085938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001210927963256836

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03476142883300781
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.031034469604492188
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011277198791503906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012335777282714844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011591911315917969
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.00119781494140625

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.027895450592041016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02451944351196289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011436939239501953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.003612041473388672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0023775100708007812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0024902820587158203

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 68 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029342174530029297
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.023056507110595703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011594295501708984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012052059173583984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00231170654296875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.006592750549316406

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0322270393371582
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018702268600463867
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012848377227783203
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.003123044967651367
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016345977783203125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0022079944610595703

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 70 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029761552810668945
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015826940536499023
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011749267578125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011758804321289062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.012427330017089844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0019011497497558594

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 71 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03179931640625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.014985084533691406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.004607677459716797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012562274932861328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.007069587707519531
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016117095947265625

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.029163837432861328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.030008316040039062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011289119720458984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0031442642211914062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001870870590209961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0018856525421142578

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 73 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.030333518981933594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.029663801193237305
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001421213150024414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0044252872467041016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001287221908569336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013890266418457031

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 74 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.025763511657714844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.017725229263305664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014281272888183594
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0010824203491210938
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012216567993164062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.003192424774169922

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03255414962768555
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.036606788635253906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013577938079833984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012264251708984375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001271963119506836
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0067729949951171875

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.021642684936523438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03328275680541992
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.003213644027709961
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012378692626953125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.002458333969116211
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016503334045410156

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02709341049194336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.027661800384521484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.004331111907958984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0014560222625732422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0023500919342041016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0015225410461425781

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02725386619567871
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.024935483932495117
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010938644409179688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011334419250488281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0021123886108398438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013360977172851562

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02530384063720703
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.036006927490234375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0016765594482421875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0016257762908935547
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012176036834716797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012392997741699219

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 80 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.016289234161376953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015139341354370117
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012192726135253906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001154184341430664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00106048583984375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0034580230712890625

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 81 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.01725029945373535
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01847553253173828
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013883113861083984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012335777282714844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0014770030975341797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004213571548461914

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 82 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0249025821685791
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.0199737548828125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0008611679077148438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0009062290191650391
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0009043216705322266
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009558200836181641

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 83 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02454972267150879
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.024229764938354492
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014386177062988281
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.002588510513305664
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0014100074768066406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012638568878173828

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.028093338012695312
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02915501594543457
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013124942779541016
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0013098716735839844
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012562274932861328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0009717941284179688

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.04214215278625488
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.03271198272705078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0012650489807128906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011172294616699219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011823177337646484
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001271963119506836

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0298919677734375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.022937536239624023
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011429786682128906
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001680135726928711
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0049250125885009766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.007466793060302734

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02986621856689453
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.034247636795043945
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013835430145263672
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011630058288574219
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0016841888427734375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013859272003173828

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.026553869247436523
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.023233413696289062
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014026165008544922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011713504791259766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012786388397216797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012547969818115234

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 89 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.02910614013671875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02221965789794922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0010058879852294922
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011496543884277344
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012073516845703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.005689859390258789

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.0287473201751709
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.018706083297729492
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001360177993774414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0020732879638671875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.00125885009765625
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.004362344741821289

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.017866134643554688
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.032263994216918945
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0021953582763671875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0011358261108398438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0011668205261230469
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0013911724090576172

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 92 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.028293132781982422
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.009120941162109375
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011866092681884766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0012257099151611328
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.004034280776977539
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0012962818145751953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Warning: environment still referenced so free is deferred (Continue to use WLS)
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.045990705490112305
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.025611162185668945
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0011255741119384766
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.00594782829284668
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.001813650131225586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0014655590057373047

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03118133544921875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.021494150161743164
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0023255348205566406
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001695871353149414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.003984689712524414
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0027618408203125

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.03135251998901367
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015610933303833008
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.001506805419921875
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.003170490264892578
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0021674633026123047
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016930103302001953

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 96 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.028772354125976562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.01830434799194336
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0013897418975830078
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001905202865600586
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0014104843139648438
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.001470804214477539

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.015646696090698242
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.015497207641601562
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0014793872833251953
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.0015735626220703125
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0015535354614257812
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.0016751289367675781

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
    [Inactive neuron in Nap] Layer 2, Neuron 4
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.0000, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[INFO] Coarsened NAP at iteration 98 was verified as robust.
[INFO] Loading ONNX model from tools/mnist-10x2.onnx
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 3: 0.021909236907958984
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 5: 0.02250361442565918
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 7: 0.0015969276428222656
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 9: 0.001859903335571289
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 11: 0.0012633800506591797
<class 'plnn.proxlp_solver.propagation.Propagation'> Time used for layer 13: 0.006291389465332031

[DEBUG] Layer-wise bound shapes BEFORE NAP:
  Layer 0: LB shape = torch.Size([1, 784]), UB shape = torch.Size([1, 784])
  Layer 1: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 2: LB shape = torch.Size([1, 10]), UB shape = torch.Size([1, 10])
  Layer 3: LB shape = torch.Size([1, 9]), UB shape = torch.Size([1, 9])
  Layer 4: LB shape = torch.Size([1, 5]), UB shape = torch.Size([1, 5])
  Layer 5: LB shape = torch.Size([1, 3]), UB shape = torch.Size([1, 3])
  Layer 6: LB shape = torch.Size([1, 2]), UB shape = torch.Size([1, 2])
  Layer 7: LB shape = torch.Size([1, 1]), UB shape = torch.Size([1, 1])
[DEBUG] Total bound layers: 8 (includes input and output)
[DEBUG] Intermidiate Layer 1: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 1 th relu layer 
[DEBUG] Now working on the 1 th neuron of 1 th relu layer 
    [Inactive neuron in Nap] Layer 1, Neuron 1
[DEBUG] Now working on the 2 th neuron of 1 th relu layer 
[DEBUG] Now working on the 3 th neuron of 1 th relu layer 
[DEBUG] Now working on the 4 th neuron of 1 th relu layer 
[DEBUG] Now working on the 5 th neuron of 1 th relu layer 
[DEBUG] Now working on the 6 th neuron of 1 th relu layer 
[DEBUG] Now working on the 7 th neuron of 1 th relu layer 
[DEBUG] Now working on the 8 th neuron of 1 th relu layer 
[DEBUG] Now working on the 9 th neuron of 1 th relu layer 
[DEBUG] Intermidiate Layer 2: NAP length = 10, Bounds neurons = 10
[DEBUG] Now working on the 0 th neuron of 2 th relu layer 
[DEBUG] Now working on the 1 th neuron of 2 th relu layer 
[DEBUG] Now working on the 2 th neuron of 2 th relu layer 
[DEBUG] Now working on the 3 th neuron of 2 th relu layer 
    [ACTive neuron in Nap] Layer 2, Neuron 3
[DEBUG] Now working on the 4 th neuron of 2 th relu layer 
[DEBUG] Now working on the 5 th neuron of 2 th relu layer 
[DEBUG] Now working on the 6 th neuron of 2 th relu layer 
[DEBUG] Now working on the 7 th neuron of 2 th relu layer 
[DEBUG] Now working on the 8 th neuron of 2 th relu layer 
[DEBUG] Now working on the 9 th neuron of 2 th relu layer 
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.
NAP update only for inner bounds, skipping because this is either an  output or/an extra verif layer.

[DEBUG] Layer-wise bounds AFTER applying NAP:
  Layer 0: LB min=-1.0000, max=-1.0000 | UB min=1.0000, max=1.0000
  Layer 1: LB min=-8.9042, max=6.6481 | UB min=-2.6785, max=12.7221
  Layer 2: LB min=-8.9977, max=8.7622 | UB min=0.2414, max=17.7282
  Layer 3: LB min=-28.8870, max=36.1114 | UB min=0.0000, max=61.5881
  Layer 4: LB min=-17.1108, max=67.9925 | UB min=11.6130, max=91.9514
  Layer 5: LB min=-13.6868, max=65.5930 | UB min=20.9571, max=102.5686
  Layer 6: LB min=-11.9527, max=36.1114 | UB min=28.9650, max=61.5881
  Layer 7: LB min=-20.0080, max=-20.0080 | UB min=21.2813, max=21.2813
Set parameter WLSAccessID
Set parameter WLSSecret
Set parameter LicenseID to value 2684372
Academic license 2684372 - for non-commercial use only - registered to sa___@bordeaux-inp.fr
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
Running Nb states visited: 0.0
[Stochastic] Coarsened neurons: 17 | Remaining %: 15.00 | successful_iterations=44
[INFO] Coarsened NAP: [[-1, 0, -1, -1, -1, -1, -1, -1, -1, -1], [-1, -1, -1, 1, 0, -1, -1, -1, -1, -1]]
[INFO] Original NAP:  [[1, 0, 1, 1, 0, 0, 1, 1, 0, 1], [1, 1, 1, 1, 0, 1, 1, 1, 0, 1]]
[OK] Results saved to /home/goofy/stage/modified-oval-bab/clean/experiments/first_script+20250811_131819/results.json
